# -*- coding: utf-8 -*-
"""DQN Bandits - Tf1 Session graph.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RW_5MBKd03g3HHpllZjx4CYD2JvFk0IU

Implementation of Deep Q Network in Tensorflow 1 using Session graph model
"""

import tensorflow.compat.v1 as tf
tf.disable_v2_behavior()
import numpy as np
import random 
from collections import defaultdict
import matplotlib.pyplot as plt
import tensorflow.keras.layers as layers

"""Bandit distribution functions:"""

def get_reward(idx, mu_sigma_dict):
    mu, sigma = mu_sigma_dict[idx]
    return np.random.normal(mu, sigma)

def generate_data(num_buttons):
    mu_sigma_dict = defaultdict(tuple)
    for i in range(num_buttons):
        mean = random.uniform(0, 10)
        sigma = random.uniform(0, 2)
        mu_sigma_dict[i] = (mean, sigma)
    print("Distribution (unknown) : {} \n".format(mu_sigma_dict))
    return mu_sigma_dict

def get_optimum_reward(mu_sigma_dict, trials):
    mean_list = [value[0] for value in mu_sigma_dict.values()]
    max_mean_idx = np.argmax(mean_list)
    optimum_reward = np.sum([get_reward(max_mean_idx, mu_sigma_dict) for _ in range(trials)])

    return optimum_reward

num_bandits = 5
epsilon = 0.2
beta = 0.1
total_steps = 1000

generate_data(num_bandits)

def DQN(num_bandits, epsilon = 0.2, beta = 0.1, total_steps = 1000):

  layer_init = tf.variance_scaling_initializer()
  act_fn = tf.nn.relu

  # Model 
  bandit_inputs = tf.placeholder(shape=[None, num_bandits, 2], dtype=tf.float32 )
  flatten = layers.Flatten()(bandit_inputs)
  dense1 = layers.Dense( units = 100, activation = act_fn , kernel_initializer = layer_init, bias_initializer = layer_init )(flatten)
  dense2 = layers.Dense( units = 50, activation = act_fn , kernel_initializer = layer_init, bias_initializer = layer_init )(dense1)
  Q_vals = layers.Dense( units = num_bandits, activation = None , kernel_initializer = layer_init, bias_initializer = layer_init)(dense2)

  Q_max = tf.reduce_max(Q_vals, axis = 1, keepdims = True)
  Q_action = tf.argmax( Q_vals, axis = 1 )  

  # Compute Q-Updates
  reward = tf.placeholder(shape=[None, 1],dtype=tf.float32)
  Q_update = reward + beta * Q_max

  computed_Q_update_value = tf.placeholder(shape=[None,1],dtype=tf.float32)

  action_selected = tf.placeholder(shape=[None,1], dtype=tf.int32)
  bandit_selected = tf.one_hot( action_selected, depth = num_bandits )
  Q_selected = tf.reduce_sum( Q_vals * bandit_selected, axis = 1, keepdims = True)

  # Computing loss
  Q_loss = tf.reduce_mean( (Q_selected - computed_Q_update_value)**2)
  optimizer = tf.train.AdamOptimizer()
  train = optimizer.minimize(Q_loss)

  # Initialize environment, state
  mu_sigma_dict = generate_data(num_bandits)
  total_reward = np.zeros(num_bandits)
  state = np.zeros((num_bandits, 2))

  state_vec = []
  Q_update_vec = []
  action_vec = []

  # Initializing session graph
  init = tf.global_variables_initializer()
  with tf.Session() as sess:
        sess.run(init)
        i = 0

        # Running t timesteps
        while i < total_steps:
         
          # Choose action using e-greedy
          if np.random.rand(1) < epsilon:
              a_t = np.random.randint(num_bandits)
          else:
              action = sess.run([Q_action], feed_dict = {bandit_inputs: [state]})
              a_t = action[0].item()

          # Generating reward based on selected action
          r_t = get_reward(a_t, mu_sigma_dict)
          # print("Reward at t:", r_t)
          total_reward[a_t] += r_t


          # New state t+1 
          X_mu = state[a_t][0]
          X_n = state[a_t][1]

          next_state = np.copy(state)
          next_state[a_t][0] = (X_mu * X_n + r_t) / (X_n + 1)
          next_state[a_t][1] = X_n + 1
          
          # Compuating target (Q update value)
          Q_update_values = sess.run(Q_update, 
                                     feed_dict = { reward : [np.reshape(r_t, (1))], bandit_inputs : [next_state] } )

          Q_update_values = Q_update_values[0]
          # print(type(Q_update_values))
          
          state_vec.append(state)
          Q_update_vec.append(Q_update_values)
          action_vec.append(a_t)

          # Batch training after 10 timesteps
          if (i + 1) % 10 == 0:
            Q_update_vec = np.array(Q_update_vec)
            # print(Q_update_vec.shape)

            action_vec = np.array(action_vec)
            # print(action_vec.shape)

            state_vec = np.array(state_vec)
            # print(state_vec.shape)

            # print("Time step:", i)
            _, loss = sess.run( [train, Q_loss], feed_dict = { computed_Q_update_value : Q_update_vec, 
                                                action_selected : np.reshape(action_vec, (-1,1)),
                                                bandit_inputs : state_vec})
            state_vec = []
            Q_update_vec = []
            action_vec = []

          state = np.copy(next_state)
          i += 1
          # print("****************************************")

DQN(num_bandits, epsilon = 0.2, beta = 0.1, total_steps = 1000)

"""To add:
Regret computation and averaging result across episodes.
"""

