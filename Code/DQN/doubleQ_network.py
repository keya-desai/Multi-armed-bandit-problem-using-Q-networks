# -*- coding: utf-8 -*-
"""Double QN Tf2.ipynb

Automatically generated by Colaboratory.

Original file is located at
	https://colab.research.google.com/drive/1O8a3uXxtVfI4CCKKlk4VhuabnIlChQoT

Double Q Network with TensorFlow 2
"""

import tensorflow as tf
import numpy as np
import random 
from collections import defaultdict
import matplotlib.pyplot as plt
import tensorflow.keras.layers as layers
from q_network import Solver

class DoubleQModel(Solver):
	def __init__(self, bandit, episodes, time_steps, trials, epsilon, beta):
		super().__init__(bandit)
		self.episodes = episodes
		self.time_steps = time_steps
		self.trials = trials
		self.epsilon = epsilon
		self.beta = beta

		##############################################################################################
		layer_init = tf.keras.initializers.VarianceScaling()
		act_fn = tf.nn.relu
		num_bandits = bandit.k
		bandit_inputs = layers.Input( shape = (num_bandits,2) )
		flatten = layers.Flatten()( bandit_inputs )

		####################################     Q_A           #################################################################################
		dense_1a = layers.Dense( units = 100, activation = act_fn , kernel_initializer = layer_init, bias_initializer = layer_init )(flatten)
		dense_2a = layers.Dense( units = 50, activation = act_fn , kernel_initializer = layer_init, bias_initializer = layer_init )(dense_1a)
		Qa_vals = layers.Dense( units = num_bandits, activation = None , kernel_initializer = layer_init, bias_initializer = layer_init)(dense_2a)
		self.Qa_value_compute = tf.keras.Model( inputs = bandit_inputs, outputs = Qa_vals )

		#####################################    Q_B               #############################################################################

		dense_1b = layers.Dense( units = 100, activation = act_fn , kernel_initializer = layer_init, bias_initializer = layer_init )(flatten)
		dense_2b = layers.Dense( units = 50, activation = act_fn , kernel_initializer = layer_init, bias_initializer = layer_init )(dense_1b)
		Qb_vals = layers.Dense( units = num_bandits, activation = None , kernel_initializer = layer_init, bias_initializer = layer_init)(dense_2b)
		self.Qb_value_compute = tf.keras.Model( inputs = bandit_inputs, outputs = Qb_vals )

		#########################################################################################################################################

		action_inputs = layers.Input( shape = (num_bandits,) )

		selected_Qa_value = layers.Dot( axes = 1 )( [ Qa_vals, action_inputs ] )
		self.Qa_value_selected = tf.keras.Model( inputs = [ bandit_inputs, action_inputs ], outputs = selected_Qa_value )
		self.Qa_value_selected.compile( optimizer = 'adam', loss = 'mean_squared_error' )

		selected_Qb_value = layers.Dot( axes = 1 )( [ Qb_vals, action_inputs ] )
		self.Qb_value_selected = tf.keras.Model( inputs = [ bandit_inputs, action_inputs ], outputs = selected_Qb_value )
		self.Qb_value_selected.compile( optimizer = 'adam', loss = 'mean_squared_error' )
		#########################################################################################################################################

	def train_on_one_pass( self, bandit):

		num_bandits = bandit.k

		current_state = np.zeros((num_bandits, 2))
		network = 'A'
		
		for t in range(self.time_steps):    
			# Choose network A or B
			if np.random.rand(1) < 0.5:
			  network = 'A'
			else:
			  network = 'B'

			if random.random() < self.epsilon:
				action_selected = np.random.randint(num_bandits)
			else:
				if network == 'A':
				  q_values = self.Qa_value_compute.predict(np.asarray([current_state]))[0]
				else:
				  q_values = self.Qb_value_compute.predict(np.asarray([current_state]))[0]
				action_selected = np.argmax( q_values )
			
			action_encoded = np.asarray( [ tf.keras.utils.to_categorical( action_selected, num_bandits ) ] )
			reward = bandit.generate_reward(action_selected)
			
			bandit_mean  = current_state[ action_selected ][0]
			bandit_count = current_state[ action_selected ][1]
			
			next_state = np.copy(current_state)
			next_state[ action_selected ][0] = (bandit_mean * bandit_count + reward) / (bandit_count + 1)
			next_state[ action_selected ][1] = bandit_count + 1
			
			if network == 'A':
			  next_q_value = np.max( self.Qb_value_compute.predict( np.asarray( [ next_state ] ) )[ 0 ] )
			else:
			  next_q_value = np.max( self.Qa_value_compute.predict( np.asarray( [ next_state ] ) )[ 0 ] )

			q_update_value = reward + self.beta * next_q_value
			state_input = np.asarray([current_state])
			
			if network == 'A':
			  self.Qa_value_selected.fit( [ state_input, action_encoded ],  np.asarray( [ q_update_value ] ) , epochs = 1, verbose = False ) 
			else:
			  self.Qb_value_selected.fit( [ state_input, action_encoded ],  np.asarray( [ q_update_value ] ) , epochs = 1, verbose = False )  
			
			current_state = np.copy(next_state)    


	def generate_sample_regret_trajectory(self, bandit):

		max_reward = bandit.max_mean
		num_bandits = bandit.k
		current_state = np.zeros((num_bandits, 2))
		rewards_generated = list()
		
		for t in range(self.time_steps):
			Qa_val = self.Qa_value_compute.predict( np.asarray( [ current_state ] ) )[ 0 ] 
			Qb_val = self.Qb_value_compute.predict( np.asarray( [ current_state ] ) )[ 0 ] 
			q_values = (Qa_val + Qb_val)/2
			action_selected = np.argmax( q_values )
			action_encoded = tf.keras.utils.to_categorical( action_selected, num_bandits )
			
			reward = bandit.generate_reward(action_selected)
			rewards_generated.append( max_reward - reward )
			
			bandit_mean  = current_state[ action_selected ][0]
			bandit_count = current_state[ action_selected ][1]
			
			next_state = np.copy( current_state )
			next_state[ action_selected ][0] = (bandit_mean * bandit_count + reward) / (bandit_count + 1)
			next_state[ action_selected ][1] = bandit_count + 1
			
			current_state = np.copy(next_state)    
		return rewards_generated